{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, you should fill in these paths\n",
    "_local = \"Path that points to this repository's clone\"\n",
    "Teacher_path = \"Path that points to the teacher model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import sys\n",
    "sys.path.append(_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the teacher model\n",
    "from transformers import CamembertForSequenceClassification\n",
    "Teacher_model = CamembertForSequenceClassification.from_pretrained(Teacher_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "from dataset.tanda_dataset import TandaDataset\n",
    "\n",
    "# The dataset can be loaded from a list of paragraphs where each paragraph is formated this way\n",
    "sample_paragraph = {\n",
    "    'questions' : ['Is this a question?'],\n",
    "    'passages' : [\n",
    "        'Yes, this is a question.',\n",
    "        'This passage is not an answer, but it can be used to form a tanda couple.',\n",
    "    ],\n",
    "    'answer_ids' : [0]\n",
    "}\n",
    "\n",
    "# Or from a squad-like file, such as Piaf or FQuAD\n",
    "Dataset = TandaDataset.from_squad_like('A path to a squad-like file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "\n",
    "# For reproducibility's sake, let's input a seed\n",
    "Seed = 42\n",
    "\n",
    "# Here, Train will be 60% of the Dataset's total size\n",
    "Train, ValAndTest = Dataset.split(0.6, Seed)\n",
    "# And Val and Test will each be 20% of the Dataset's total size\n",
    "Val, Test = ValAndTest.split(0.5, Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset\n",
    "# Tokenization is always done beforehand\n",
    "\n",
    "from transformers import CamembertTokenizer\n",
    "\n",
    "Train.tokenize(CamembertTokenizer.from_pretrained('camembert-base'), max_length=256)\n",
    "Val.tokenize(CamembertTokenizer.from_pretrained('camembert-base'), max_length=256)\n",
    "Test.tokenize(CamembertTokenizer.from_pretrained('camembert-base'), max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actual distillation model\n",
    "\n",
    "from models.distilCamembertForSequenceClassification import DistilledCamembertForSequenceClassification\n",
    "\n",
    "# To understand this parameter, look up softmax temperature\n",
    "# You can leave this at 1\n",
    "# The higher the temperature, the lower the models' confidences\n",
    "SoftmaxTemperature = 2\n",
    "\n",
    "DistilledCamembert = DistilledCamembertForSequenceClassification(\n",
    "    teacher=Teacher_model, \n",
    "    temperature=SoftmaxTemperature, \n",
    "    )\n",
    "    \n",
    "DistilledCamembert.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A logger to log in the model's training\n",
    "\n",
    "from logger import Logger as LoggerClass\n",
    "\n",
    "# For more informations on available metrics, look up the parse function in metrics.__init__\n",
    "Logger = LoggerClass(metric_descriptions=['train loss', 'train accuracy', 'train f1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting method \n",
    "\n",
    "import torch\n",
    "\n",
    "DistilledCamembert.fit(\n",
    "    # Classic torch optimizer\n",
    "    optimizer=torch.optim.AdamW(params=DistilledCamembert.parameters(), lr=1e-5), \n",
    "    # Logger\n",
    "    logger=Logger, \n",
    "    # Number of epochs\n",
    "    epochs=10,\n",
    "    # Training dataset\n",
    "    training_dataset=Train,\n",
    "    # Validation dataset (optional)\n",
    "    validation_dataset=Val,\n",
    "    # Forces the datasets to yeild 50% of couple labeled 0 and 50% labeled 1\n",
    "    force_balance=True,\n",
    "    # Batch size\n",
    "    batch_size=8,\n",
    "    # Frequency on which to do backpropagation, if =2 then there is backpropagation every 2 batches\n",
    "    backpropagation_frequency = 1,\n",
    "    # Whether or not to run a validation before the first epoch\n",
    "    pre_training_validation=False,\n",
    "    # Maximum length of each input\n",
    "    max_length=128,\n",
    "    # Display progress at the end of each epoch or not\n",
    "    verbose=True,\n",
    "    # You can set a new temperature if it hasn't been done yet\n",
    "    temperature=2,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "07d2458d2d97685183d3136e32d35855b2fab2257fccd11b49fd0462e7835ec0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('rorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
